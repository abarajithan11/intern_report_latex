
@article{yolo,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.02640},
  primaryClass = {cs},
  title = {You {{Only Look Once}}: {{Unified}}, {{Real}}-{{Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  journal = {arXiv:1506.02640 [cs]},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  month = jun,
  year = {2015},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\\Users\\abara\\Zotero\\storage\\Z3HHNFPL\\Redmon et al. - 2015 - You Only Look Once Unified, Real-Time Object Dete.pdf;C:\\Users\\abara\\Zotero\\storage\\3BJ3NMPP\\1506.html}
}

@misc{jetson_tx2,
  title = {Jetson {{TX2 Module}}},
  language = {en},
  journal = {Jetson TX2 Module},
  howpublished = {https://developer.nvidia.com/embedded/buy/jetson-tx2},
  month = may,
  year = {2017},
  file = {C:\\Users\\abara\\Zotero\\storage\\2QZ8Z5C9\\jetson-tx2.html}
}

@misc{presentation,
  title = {Presentation: {{ML Pipeline}} - {{DATA61 RRG}}},
  abstract = {END TO END PIPELINE for Machine Learning in Robotics Demonstrated through vision-based autonomous navigation Aba | Uvindu Supervised by Dr. Nicholas Hudson Co-supervised by Dr. Navinda Kottege},
  language = {en-GB},
  journal = {Google Docs},
  howpublished = {docs.google.com/presentation/d/1Z7OW8ILDy-wdoWo1Hw84-F0VBjzLAXzpfrpAYbkIddE/},
  author = {Gnaneswaran, Abarajithan and Perera, Uvindu},
  file = {C:\\Users\\abara\\Zotero\\storage\\MAA3B52L\\edit.html}
}

@article{trailnet,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.02550},
  primaryClass = {cs},
  title = {Toward {{Low}}-{{Flying Autonomous MAV Trail Navigation}} Using {{Deep Neural Networks}} for {{Environmental Awareness}}},
  abstract = {We present a micro aerial vehicle (MAV) system, built with inexpensive off-the-shelf hardware, for autonomously following trails in unstructured, outdoor environments such as forests. The system introduces a deep neural network (DNN) called TrailNet for estimating the view orientation and lateral offset of the MAV with respect to the trail center. The DNN-based controller achieves stable flight without oscillations by avoiding overconfident behavior through a loss function that includes both label smoothing and entropy reward. In addition to the TrailNet DNN, the system also utilizes vision modules for environmental awareness, including another DNN for object detection and a visual odometry component for estimating depth for the purpose of low-level obstacle detection. All vision systems run in real time on board the MAV via a Jetson TX1. We provide details on the hardware and software used, as well as implementation details. We present experiments showing the ability of our system to navigate forest trails more robustly than previous techniques, including autonomous flights of 1 km.},
  journal = {arXiv:1705.02550 [cs]},
  author = {Smolyanskiy, Nikolai and Kamenev, Alexey and Smith, Jeffrey and Birchfield, Stan},
  month = may,
  year = {2017},
  keywords = {Computer Science - Robotics},
  file = {C:\\Users\\abara\\Zotero\\storage\\CVSWNLFS\\Smolyanskiy et al. - 2017 - Toward Low-Flying Autonomous MAV Trail Navigation .pdf;C:\\Users\\abara\\Zotero\\storage\\SFSBUVGZ\\1705.html}
}

@article{resnet,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1512.03385},
  primaryClass = {cs},
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  journal = {arXiv:1512.03385 [cs]},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  month = dec,
  year = {2015},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\\Users\\abara\\Zotero\\storage\\6KNVEFXN\\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf;C:\\Users\\abara\\Zotero\\storage\\J3D3AWCY\\1512.html}
}

@misc{tfrecords,
  title = {Using {{TFrecords}} and {{TFexamples}}},
  language = {en},
  journal = {Using TFRecords and tf.Example},
  howpublished = {https://www.tensorflow.org/tutorials/load\_data/tf-records},
  file = {C:\\Users\\abara\\Zotero\\storage\\5869JMR9\\tf-records.html}
}

@misc{darpa,
  title = {{{DARPA Subterranean Challenge}}: {{Unearthing}} the {{Subterranean Environment}}},
  shorttitle = {{{DARPA Subterranean Challenge}}},
  abstract = {The DARPA Subterranean Challenge aims to explore new approaches to rapidly map, navigate, and search underground environments.},
  language = {en},
  journal = {DARPA Subterranean Challenge},
  howpublished = {https://subtchallenge.com/},
  author = {{team}, Booz Allen DX},
  file = {C:\\Users\\abara\\Zotero\\storage\\5PLWWDNS\\subtchallenge.com.html}
}

@article{trailnet2,
  title = {A {{Machine Learning Approach}} to {{Visual Perception}} of {{Forest Trails}} for {{Mobile Robots}}},
  volume = {1},
  issn = {2377-3766},
  doi = {10.1109/LRA.2015.2509024},
  abstract = {We study the problem of perceiving forest or mountain trails from a single monocular image acquired from the viewpoint of a robot traveling on the trail itself. Previous literature focused on trail segmentation, and used low-level features such as image saliency or appearance contrast; we propose a different approach based on a deep neural network used as a supervised image classifier. By operating on the whole image at once, our system outputs the main direction of the trail compared to the viewing direction. Qualitative and quantitative results computed on a large real-world dataset (which we provide for download) show that our approach outperforms alternatives, and yields an accuracy comparable to the accuracy of humans that are tested on the same image classification task. Preliminary results on using this information for quadrotor control in unseen trails are reported. To the best of our knowledge, this is the first letter that describes an approach to perceive forest trials, which is demonstrated on a quadrotor micro aerial vehicle.},
  number = {2},
  journal = {IEEE Robotics and Automation Letters},
  author = {Giusti, A. and Guzzi, J. and Cire{\c s}an, D. C. and He, F. and Rodr\'iguez, J. P. and Fontana, F. and Faessler, M. and Forster, C. and Schmidhuber, J. and Caro, G. D. and Scaramuzza, D. and Gambardella, L. M.},
  month = jul,
  year = {2016},
  keywords = {Aerial Robotics,autonomous aerial vehicles,Cameras,Deep Learning,deep-neural network,forest trails,helicopters,image classification,Image segmentation,learning (artificial intelligence),Machine Learning,machine learning approach,microrobots,mobile robots,Mobile robots,monocular image,neural nets,quadrotor microaerial vehicle control,qualitative analysis,quantitative analysis,Roads,robot vision,Robot vision systems,supervised image classifier,viewing direction,visual perception,Visual perception,Visual-Based Navigation},
  pages = {661-667},
  file = {C:\\Users\\abara\\Zotero\\storage\\3CMXZB3L\\Giusti et al. - 2016 - A Machine Learning Approach to Visual Perception o.pdf;C:\\Users\\abara\\Zotero\\storage\\2G3VSI4B\\7358076.html}
}

@article{trailnet3,
  title = {An {{Optimised Deep Neural Network Approach}} for {{Forest Trail Navigation}} for {{UAV Operation}} within the {{Forest Canopy}}},
  abstract = {Autonomous flight within a forest canopy represents a key challenge for generalised scene understanding on-board a future Unmanned Aerial Vehicle (UAV) platform. Here we present an approach for automatic trail navigation within such an environment that successfully generalises across differing image resolutions allowing UAV with varying sensor payload capabilities to operate equally in such challenging environmental conditions. Specifically, this work presents an optimised deep neural network architecture, capable of stateof-the-art performance across varying resolution aerial UAV imagery, that improves forest trail detection for UAV guidance even when using significantly low resolution images that are representative of low-cost search and rescue capable UAV platforms.},
  language = {en},
  author = {{Maciel-Pearson}, B G and Breckon, T P},
  pages = {3},
  file = {C:\\Users\\abara\\Zotero\\storage\\XLCM9FHM\\Maciel-Pearson and Breckon - An Optimised Deep Neural Network Approach for Fore.pdf}
}

@inproceedings{trailnet4,
  series = {Lecture Notes in Computer Science},
  title = {Extending {{Deep Neural Network Trail Navigation}} for {{Unmanned Aerial Vehicle Operation Within}} the {{Forest Canopy}}},
  isbn = {978-3-319-96728-8},
  abstract = {Autonomous flight within a forest canopy represents a key challenge for generalised scene understanding on-board a future Unmanned Aerial Vehicle (UAV) platforms. Here we present an approach for automatic trail navigation within such an unstructured environment that successfully generalises across differing image resolutions - allowing UAV with varying sensor payload capabilities to operate equally in such challenging environmental conditions. Specifically, this work presents an optimised deep neural network architecture, capable of state-of-the-art performance across varying resolution aerial UAV imagery, that improves forest trail detection for UAV guidance even when using significantly low resolution images that are representative of low-cost search and rescue capable UAV platforms.},
  language = {en},
  booktitle = {Towards {{Autonomous Robotic Systems}}},
  publisher = {{Springer International Publishing}},
  author = {{Maciel-Pearson}, Bruna G. and Carbonneau, Patrice and Breckon, Toby P.},
  editor = {Giuliani, Manuel and Assaf, Tareq and Giannaccini, Maria Elena},
  year = {2018},
  keywords = {Autonomous UAV,Deep learning,Trail detection,Unstructured environment},
  pages = {147-158}
}

@misc{idsia,
  type = {Dataset},
  title = {{{IDSIA Dataset}}},
  howpublished = {http://people.idsia.ch/\textasciitilde{}guzzi/DataSet.html},
  file = {C:\\Users\\abara\\Zotero\\storage\\WJ2NEJ9J\\DataSet.html}
}

@article{hand_eye,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.02199},
  primaryClass = {cs},
  title = {Learning {{Hand}}-{{Eye Coordination}} for {{Robotic Grasping}} with {{Deep Learning}} and {{Large}}-{{Scale Data Collection}}},
  abstract = {We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images and independently of camera calibration or the current robot pose. This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination. We then use this network to servo the gripper in real time to achieve successful grasps. To train our network, we collected over 800,000 grasp attempts over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and hardware. Our experimental evaluation demonstrates that our method achieves effective real-time control, can successfully grasp novel objects, and corrects mistakes by continuous servoing.},
  journal = {arXiv:1603.02199 [cs]},
  author = {Levine, Sergey and Pastor, Peter and Krizhevsky, Alex and Quillen, Deirdre},
  month = mar,
  year = {2016},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C:\\Users\\abara\\Zotero\\storage\\V3AM6CKV\\Levine et al. - 2016 - Learning Hand-Eye Coordination for Robotic Graspin.pdf;C:\\Users\\abara\\Zotero\\storage\\JG86HCBN\\1603.html}
}

@article{distill1,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1507.00448},
  primaryClass = {cs},
  title = {Cross {{Modal Distillation}} for {{Supervision Transfer}}},
  abstract = {In this work we propose a technique that transfers supervision between images from different modalities. We use learned representations from a large labeled modality as a supervisory signal for training representations for a new unlabeled paired modality. Our method enables learning of rich representations for unlabeled modalities and can be used as a pre-training procedure for new modalities with limited labeled data. We show experimental results where we transfer supervision from labeled RGB images to unlabeled depth and optical flow images and demonstrate large improvements for both these cross modal supervision transfers. Code, data and pre-trained models are available at https://github.com/s-gupta/fast-rcnn/tree/distillation},
  journal = {arXiv:1507.00448 [cs]},
  author = {Gupta, Saurabh and Hoffman, Judy and Malik, Jitendra},
  month = jul,
  year = {2015},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\\Users\\abara\\Zotero\\storage\\G4NLT7QD\\Gupta et al. - 2015 - Cross Modal Distillation for Supervision Transfer.pdf;C:\\Users\\abara\\Zotero\\storage\\Y5ZRB8ZA\\1507.html}
}

@article{cross_modal2,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1507.06821},
  primaryClass = {cs},
  title = {Multimodal {{Deep Learning}} for {{Robust RGB}}-{{D Object Recognition}}},
  abstract = {Robust object recognition is a crucial ingredient of many, if not all, real-world robotics applications. This paper leverages recent progress on Convolutional Neural Networks (CNNs) and proposes a novel RGB-D architecture for object recognition. Our architecture is composed of two separate CNN processing streams - one for each modality - which are consecutively combined with a late fusion network. We focus on learning with imperfect sensor data, a typical problem in real-world robotics tasks. For accurate learning, we introduce a multi-stage training methodology and two crucial ingredients for handling depth data with CNNs. The first, an effective encoding of depth information for CNNs that enables learning without the need for large depth datasets. The second, a data augmentation scheme for robust learning with depth images by corrupting them with realistic noise patterns. We present state-of-the-art results on the RGB-D object dataset and show recognition in challenging RGB-D real-world noisy settings.},
  journal = {arXiv:1507.06821 [cs]},
  author = {Eitel, Andreas and Springenberg, Jost Tobias and Spinello, Luciano and Riedmiller, Martin and Burgard, Wolfram},
  month = jul,
  year = {2015},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics},
  file = {C:\\Users\\abara\\Zotero\\storage\\RRYJJJMD\\Eitel et al. - 2015 - Multimodal Deep Learning for Robust RGB-D Object R.pdf;C:\\Users\\abara\\Zotero\\storage\\77S5TKS5\\1507.html}
}

@article{mobile_cnn,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.04861},
  primaryClass = {cs},
  title = {{{MobileNets}}: {{Efficient Convolutional Neural Networks}} for {{Mobile Vision Applications}}},
  shorttitle = {{{MobileNets}}},
  abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
  journal = {arXiv:1704.04861 [cs]},
  author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  month = apr,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\\Users\\abara\\Zotero\\storage\\BCRXLK9C\\Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Network.pdf;C:\\Users\\abara\\Zotero\\storage\\2SDFW5Q3\\1704.html}
}

@article{cross_modal1,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.07616},
  primaryClass = {cs, stat},
  title = {Do {{Neural Network Cross}}-{{Modal Mappings Really Bridge Modalities}}?},
  abstract = {Feed-forward networks are widely used in cross-modal applications to bridge modalities by mapping distributed vectors of one modality to the other, or to a shared space. The predicted vectors are then used to perform e.g., retrieval or labeling. Thus, the success of the whole system relies on the ability of the mapping to make the neighborhood structure (i.e., the pairwise similarities) of the predicted vectors akin to that of the target vectors. However, whether this is achieved has not been investigated yet. Here, we propose a new similarity measure and two ad hoc experiments to shed light on this issue. In three cross-modal benchmarks we learn a large number of language-to-vision and vision-to-language neural network mappings (up to five layers) using a rich diversity of image and text features and loss functions. Our results reveal that, surprisingly, the neighborhood structure of the predicted vectors consistently resembles more that of the input vectors than that of the target vectors. In a second experiment, we further show that untrained nets do not significantly disrupt the neighborhood (i.e., semantic) structure of the input vectors.},
  journal = {arXiv:1805.07616 [cs, stat]},
  author = {Collell, Guillem and Moens, Marie-Francine},
  month = may,
  year = {2018},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\\Users\\abara\\Zotero\\storage\\RR3IZAIN\\Collell and Moens - 2018 - Do Neural Network Cross-Modal Mappings Really Brid.pdf;C:\\Users\\abara\\Zotero\\storage\\2LJSJFUS\\1805.html}
}

@article{distill2,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1503.02531},
  primaryClass = {cs, stat},
  title = {Distilling the {{Knowledge}} in a {{Neural Network}}},
  abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
  journal = {arXiv:1503.02531 [cs, stat]},
  author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  month = mar,
  year = {2015},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C:\\Users\\abara\\Zotero\\storage\\J9LF5ARZ\\Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf;C:\\Users\\abara\\Zotero\\storage\\9HS5ECHV\\1503.html}
}


